{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["-JwoFcqL9FBc","YMbIaiSiNkz1","rTLjZJ-JNVu0","CrQjfYxwN8KX","Qz34p3i1OA3R","3SSkgD37OHm3"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# í•„ìš” íŒ¨í‚¤ì§€ ì„¤ì¹˜ (streamlit, kobert)"],"metadata":{"id":"-JwoFcqL9FBc"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qONxoJQ18iuJ","outputId":"4caaf853-39fd-414c-b94c-9a7ed879e028","executionInfo":{"status":"ok","timestamp":1676120114185,"user_tz":-540,"elapsed":153835,"user":{"displayName":"JUNSEOK LEE","userId":"08889545003597910636"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.0/79.0 KB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m164.8/164.8 KB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m239.0/239.0 KB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m184.0/184.0 KB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.5/84.5 KB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython 7.9.0 requires jedi>=0.10, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting streamlit-folium\n","  Downloading streamlit_folium-0.11.0-py3-none-any.whl (423 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m423.2/423.2 KB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: streamlit>=1.2 in /usr/local/lib/python3.8/dist-packages (from streamlit-folium) (1.18.1)\n","Requirement already satisfied: branca in /usr/local/lib/python3.8/dist-packages (from streamlit-folium) (0.6.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from streamlit-folium) (2.11.3)\n","Collecting folium>=0.13\n","  Downloading folium-0.14.0-py2.py3-none-any.whl (102 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m102.3/102.3 KB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from folium>=0.13->streamlit-folium) (1.21.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from folium>=0.13->streamlit-folium) (2.25.1)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->streamlit-folium) (2.0.1)\n","Requirement already satisfied: protobuf<4,>=3.12 in /usr/local/lib/python3.8/dist-packages (from streamlit>=1.2->streamlit-folium) (3.19.6)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from streamlit>=1.2->streamlit-folium) (7.1.2)\n","Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.8/dist-packages (from streamlit>=1.2->streamlit-folium) (5.3.0)\n","Requirement already satisfied: watchdog in /usr/local/lib/python3.8/dist-packages (from streamlit>=1.2->streamlit-folium) (2.2.1)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.8/dist-packages (from streamlit>=1.2->streamlit-folium) (13.3.1)\n","Requirement already satisfied: semver in /usr/local/lib/python3.8/dist-packages (from streamlit>=1.2->streamlit-folium) (2.13.0)\n","Requirement already satisfied: blinker>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from streamlit>=1.2->streamlit-folium) (1.5)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.8/dist-packages (from streamlit>=1.2->streamlit-folium) (7.1.2)\n","Requirement already satisfied: packaging>=14.1 in /usr/local/lib/python3.8/dist-packages (from streamlit>=1.2->streamlit-folium) (23.0)\n","Requirement already satisfied: tornado>=6.0.3 in /usr/local/lib/python3.8/dist-packages (from streamlit>=1.2->streamlit-folium) (6.0.4)\n","Requirement already satisfied: pympler>=0.9 in /usr/local/lib/python3.8/dist-packages (from streamlit>=1.2->streamlit-folium) (1.0.1)\n","Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.8/dist-packages (from streamlit>=1.2->streamlit-folium) (6.0.0)\n","Requirement already satisfied: tzlocal>=1.1 in /usr/local/lib/python3.8/dist-packages (from streamlit>=1.2->streamlit-folium) (1.5.1)\n","Requirement already satisfied: gitpython!=3.1.19 in /usr/local/lib/python3.8/dist-packages (from streamlit>=1.2->streamlit-folium) (3.1.30)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from streamlit>=1.2->streamlit-folium) (2.8.2)\n","Requirement already satisfied: pydeck>=0.1.dev5 in /usr/local/lib/python3.8/dist-packages (from streamlit>=1.2->streamlit-folium) (0.8.0)\n","Requirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.8/dist-packages (from streamlit>=1.2->streamlit-folium) (4.4.0)\n","Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from streamlit>=1.2->streamlit-folium) (4.2.2)\n","Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.8/dist-packages (from streamlit>=1.2->streamlit-folium) (1.3.5)\n","Requirement already satisfied: pyarrow>=4.0 in /usr/local/lib/python3.8/dist-packages (from streamlit>=1.2->streamlit-folium) (9.0.0)\n","Requirement already satisfied: toml in /usr/local/lib/python3.8/dist-packages (from streamlit>=1.2->streamlit-folium) (0.10.2)\n","Requirement already satisfied: validators>=0.2 in /usr/local/lib/python3.8/dist-packages (from streamlit>=1.2->streamlit-folium) (0.20.0)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.8/dist-packages (from altair>=3.2.0->streamlit>=1.2->streamlit-folium) (0.4)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.8/dist-packages (from altair>=3.2.0->streamlit>=1.2->streamlit-folium) (4.3.3)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.8/dist-packages (from altair>=3.2.0->streamlit>=1.2->streamlit-folium) (0.12.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from gitpython!=3.1.19->streamlit>=1.2->streamlit-folium) (4.0.10)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=1.4->streamlit>=1.2->streamlit-folium) (3.12.1)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.25->streamlit>=1.2->streamlit-folium) (2022.7.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil->streamlit>=1.2->streamlit-folium) (1.15.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->folium>=0.13->streamlit-folium) (2022.12.7)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->folium>=0.13->streamlit-folium) (4.0.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->folium>=0.13->streamlit-folium) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->folium>=0.13->streamlit-folium) (2.10)\n","Requirement already satisfied: markdown-it-py<3.0.0,>=2.1.0 in /usr/local/lib/python3.8/dist-packages (from rich>=10.11.0->streamlit>=1.2->streamlit-folium) (2.1.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.14.0 in /usr/local/lib/python3.8/dist-packages (from rich>=10.11.0->streamlit>=1.2->streamlit-folium) (2.14.0)\n","Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from validators>=0.2->streamlit>=1.2->streamlit-folium) (4.4.2)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19->streamlit>=1.2->streamlit-folium) (5.0.0)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit>=1.2->streamlit-folium) (0.19.3)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit>=1.2->streamlit-folium) (22.2.0)\n","Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit>=1.2->streamlit-folium) (5.10.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.8/dist-packages (from markdown-it-py<3.0.0,>=2.1.0->rich>=10.11.0->streamlit>=1.2->streamlit-folium) (0.1.2)\n","Installing collected packages: folium, streamlit-folium\n","  Attempting uninstall: folium\n","    Found existing installation: folium 0.12.1.post1\n","    Uninstalling folium-0.12.1.post1:\n","      Successfully uninstalled folium-0.12.1.post1\n","Successfully installed folium-0.14.0 streamlit-folium-0.11.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyngrok\n","  Downloading pyngrok-5.2.1.tar.gz (761 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m761.3/761.3 KB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from pyngrok) (6.0)\n","Building wheels for collected packages: pyngrok\n","  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyngrok: filename=pyngrok-5.2.1-py3-none-any.whl size=19792 sha256=e08b30668f0e1203a7c161584457f1857ee917d304139026536718d171f74b48\n","  Stored in directory: /root/.cache/pip/wheels/5d/f2/70/526da675d32f17577ec47ac4c663084efe39d47c826b6c3bb1\n","Successfully built pyngrok\n","Installing collected packages: pyngrok\n","Successfully installed pyngrok-5.2.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting mxnet\n","  Downloading mxnet-1.9.1-py3-none-manylinux2014_x86_64.whl (49.1 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting graphviz<0.9.0,>=0.8.1\n","  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.8/dist-packages (from mxnet) (2.25.1)\n","Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.8/dist-packages (from mxnet) (1.21.6)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.20.0->mxnet) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.20.0->mxnet) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.20.0->mxnet) (2022.12.7)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.20.0->mxnet) (4.0.0)\n","Installing collected packages: graphviz, mxnet\n","  Attempting uninstall: graphviz\n","    Found existing installation: graphviz 0.10.1\n","    Uninstalling graphviz-0.10.1:\n","      Successfully uninstalled graphviz-0.10.1\n","Successfully installed graphviz-0.8.4 mxnet-1.9.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting gluonnlp\n","  Downloading gluonnlp-0.10.0.tar.gz (344 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m344.5/344.5 KB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (4.64.1)\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.8/dist-packages (from gluonnlp) (1.21.6)\n","Requirement already satisfied: cython in /usr/local/lib/python3.8/dist-packages (from gluonnlp) (0.29.33)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from gluonnlp) (23.0)\n","Building wheels for collected packages: gluonnlp\n","  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp38-cp38-linux_x86_64.whl size=689004 sha256=1fbc3dc37e42995d6aa1112bcdb85c1505905eec6e330e05f7bd09c19a7ef802\n","  Stored in directory: /root/.cache/pip/wheels/b6/93/9d/2237550c409eb3ed725d6302b7897ddd9a037b40cef66dcd9c\n","Successfully built gluonnlp\n","Installing collected packages: gluonnlp\n","Successfully installed gluonnlp-0.10.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.97\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n","Collecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.12.0 tokenizers-0.13.2 transformers-4.26.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n","  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-du84cr9t\n","  Running command git clone --filter=blob:none --quiet 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-du84cr9t\n","  Resolved https://****@github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting boto3<=1.15.18\n","  Downloading boto3-1.15.18-py2.py3-none-any.whl (129 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m129.1/129.1 KB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: gluonnlp<=0.10.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from kobert==0.2.3) (0.10.0)\n","Collecting mxnet<=1.7.0.post2,>=1.4.0\n","  Downloading mxnet-1.7.0.post2-py2.py3-none-manylinux2014_x86_64.whl (54.7 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.7/54.7 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting onnxruntime<=1.8.0,==1.8.0\n","  Downloading onnxruntime-1.8.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m96.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sentencepiece<=0.1.96,>=0.1.6\n","  Downloading sentencepiece-0.1.96-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch<=1.10.1,>=1.7.0\n","  Downloading torch-1.10.1-cp38-cp38-manylinux1_x86_64.whl (881.9 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m881.9/881.9 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting transformers<=4.8.1,>=4.8.1\n","  Downloading transformers-4.8.1-py3-none-any.whl (2.5 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.8/dist-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (1.21.6)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.8/dist-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (3.19.6)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.8/dist-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (23.1.21)\n","Collecting jmespath<1.0.0,>=0.7.1\n","  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n","Collecting botocore<1.19.0,>=1.18.18\n","  Downloading botocore-1.18.18-py2.py3-none-any.whl (6.7 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0\n","  Downloading s3transfer-0.3.7-py2.py3-none-any.whl (73 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m73.4/73.4 KB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: cython in /usr/local/lib/python3.8/dist-packages (from gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (0.29.33)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (23.0)\n","Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (0.8.4)\n","Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.8/dist-packages (from mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2.25.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch<=1.10.1,>=1.7.0->kobert==0.2.3) (4.4.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (2022.6.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (4.64.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (3.9.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (6.0)\n","Collecting huggingface-hub==0.0.12\n","  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: urllib3<1.26,>=1.20 in /usr/local/lib/python3.8/dist-packages (from botocore<1.19.0,>=1.18.18->boto3<=1.15.18->kobert==0.2.3) (1.24.3)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.8/dist-packages (from botocore<1.19.0,>=1.18.18->boto3<=1.15.18->kobert==0.2.3) (2.8.2)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2022.12.7)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (4.0.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (1.2.0)\n","Building wheels for collected packages: kobert, sacremoses\n","  Building wheel for kobert (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for kobert: filename=kobert-0.2.3-py3-none-any.whl size=15708 sha256=3c8c235f6468b31e65272d8c566a5c5c5a54a7e531431d9c1f014a5c13edde22\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-8622aa_i/wheels/bf/5f/74/81bf3a1332130eb6629ecf58876a8746b77021e7d7b0638e91\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=2a5fe2e77361e36a19e18d86d31b4d78bea43854e58a4587a6ab0ca78a321512\n","  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n","Successfully built kobert sacremoses\n","Installing collected packages: tokenizers, sentencepiece, torch, sacremoses, onnxruntime, jmespath, mxnet, huggingface-hub, botocore, transformers, s3transfer, boto3, kobert\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.13.2\n","    Uninstalling tokenizers-0.13.2:\n","      Successfully uninstalled tokenizers-0.13.2\n","  Attempting uninstall: sentencepiece\n","    Found existing installation: sentencepiece 0.1.97\n","    Uninstalling sentencepiece-0.1.97:\n","      Successfully uninstalled sentencepiece-0.1.97\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.13.1+cu116\n","    Uninstalling torch-1.13.1+cu116:\n","      Successfully uninstalled torch-1.13.1+cu116\n","  Attempting uninstall: mxnet\n","    Found existing installation: mxnet 1.9.1\n","    Uninstalling mxnet-1.9.1:\n","      Successfully uninstalled mxnet-1.9.1\n","  Attempting uninstall: huggingface-hub\n","    Found existing installation: huggingface-hub 0.12.0\n","    Uninstalling huggingface-hub-0.12.0:\n","      Successfully uninstalled huggingface-hub-0.12.0\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.26.1\n","    Uninstalling transformers-4.26.1:\n","      Successfully uninstalled transformers-4.26.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.14.1+cu116 requires torch==1.13.1, but you have torch 1.10.1 which is incompatible.\n","torchtext 0.14.1 requires torch==1.13.1, but you have torch 1.10.1 which is incompatible.\n","torchaudio 0.13.1+cu116 requires torch==1.13.1, but you have torch 1.10.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed boto3-1.15.18 botocore-1.18.18 huggingface-hub-0.0.12 jmespath-0.10.0 kobert-0.2.3 mxnet-1.7.0.post2 onnxruntime-1.8.0 s3transfer-0.3.7 sacremoses-0.0.53 sentencepiece-0.1.96 tokenizers-0.10.3 torch-1.10.1 transformers-4.8.1\n"]}],"source":["!pip install streamlit -q\n","!pip install streamlit-folium\n","!pip install pyngrok\n","\n","!pip install mxnet\n","!pip install gluonnlp tqdm\n","!pip install sentencepiece\n","!pip install transformers\n","!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"]},{"cell_type":"markdown","source":["# drive mount"],"metadata":{"id":"YMbIaiSiNkz1"}},{"cell_type":"code","source":["#êµ¬ê¸€ë“œë¼ì´ë¸Œ ì—°ë™\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5jgtilUKhgQn","outputId":"beb57cc9-eaf4-4e91-b3c6-c668e06694c4","executionInfo":{"status":"ok","timestamp":1676120150248,"user_tz":-540,"elapsed":20338,"user":{"displayName":"JUNSEOK LEE","userId":"08889545003597910636"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# ngrok ê°œì¸ í† í° ê°€ì ¸ì˜¤ê¸°\n","https://ngrok.com/"],"metadata":{"id":"rTLjZJ-JNVu0"}},{"cell_type":"code","source":["from pyngrok import ngrok\n","\n","ngrok.set_auth_token('2LWl68KiZpy9kzNuyRw1SdyDsVE_65GGPtib1vo8D2HTLxMDJ')"],"metadata":{"id":"XZ766ErF9i8i","executionInfo":{"status":"ok","timestamp":1676120155788,"user_tz":-540,"elapsed":2690,"user":{"displayName":"JUNSEOK LEE","userId":"08889545003597910636"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f56a0013-c9ba-4deb-cb3e-a44cb97da007"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":[]}]},{"cell_type":"markdown","source":["# py íŒŒì¼ ë§Œë“¤ê¸° (stremlit ì—°ê²°)\n","py ì €ì¥í•˜ê³  streamlit ì›¹ì—ì„œ reload\n","\n","ì½”ë“œ ìˆ˜ì •í• ë–„ : *%%writefile app.py* --> ì£¼ì„ì²˜ë¦¬ í•´ì•¼ ì½”ë“œë§ˆë‹¤ ìƒ‰ê¹” ë³´ì„"],"metadata":{"id":"kn285XHLNnh6"}},{"cell_type":"code","source":["%%writefile app2.py\n","\n","import streamlit as st\n","import streamlit.components.v1 as html\n","import numpy as np\n","import pandas as pd\n","import requests\n","import folium\n","from folium.plugins import MiniMap\n","from streamlit_folium import st_folium\n","\n","# torch\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import gluonnlp as nlp\n","import numpy as np\n","from tqdm import tqdm, tqdm_notebook\n","\n","# kobert\n","from kobert.utils import get_tokenizer\n","from kobert.pytorch_kobert import get_pytorch_kobert_model\n","\n","#transformers\n","from transformers import AdamW\n","from transformers.optimization import get_cosine_schedule_with_warmup\n","\n","# í˜ì´ì§€ì˜ ê¸°ë³¸ ì„¤ì • êµ¬ì„±\n","st.set_page_config(\n"," layout=\"wide\",\n"," page_title='ì˜¤ëŠ˜ ì´ê±° ë¨¹ì–´')\n","\n","#######################################################################################################\n","#### ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ####\n","\n","device = torch.device(\"cuda:0\") #GPUì‚¬ìš©\n","#device = torch.device(\"cpu\")  #CPUì‚¬ìš©\n","\n","bertmodel, vocab = get_pytorch_kobert_model()\n","\n","tokenizer = get_tokenizer()\n","tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n","\n","class BERTDataset(Dataset):\n","    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len, pad, pair):\n","        transform = nlp.data.BERTSentenceTransform(\n","            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n","\n","        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n","        self.labels = [np.int32(i[label_idx]) for i in dataset]\n","\n","    def __getitem__(self, i):\n","        return (self.sentences[i] + (self.labels[i], ))\n","\n","    def __len__(self):\n","        return (len(self.labels))\n","\n","max_len = 64\n","batch_size = 64\n","warmup_ratio = 0.1\n","num_epochs = 20\n","max_grad_norm = 1\n","log_interval = 200\n","learning_rate =  5e-5\n","\n","class BERTClassifier(nn.Module):\n","    def __init__(self,\n","                 bert,\n","                 hidden_size = 768,\n","                 num_classes=17,\n","                 dr_rate=None,\n","                 params=None):\n","        super(BERTClassifier, self).__init__()\n","        self.bert = bert\n","        self.dr_rate = dr_rate\n","                 \n","        self.classifier = nn.Linear(hidden_size , num_classes)\n","        if dr_rate:\n","            self.dropout = nn.Dropout(p=dr_rate)\n","    \n","    def gen_attention_mask(self, token_ids, valid_length):\n","\n","        attention_mask = torch.zeros_like(token_ids)\n","        for i, v in enumerate(valid_length):\n","            attention_mask[i][:v] = 1\n","        return attention_mask.float()\n","\n","    def forward(self, token_ids, valid_length, segment_ids):\n","        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n","\n","        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device), return_dict=False)\n","\n","        if self.dr_rate:\n","            out = self.dropout(pooler)\n","        return self.classifier(out)\n","\n","model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)\n","\n","no_decay = ['bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0} ]\n","\n","optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n","loss_fn = nn.CrossEntropyLoss()\n","\n","def calc_accuracy(X,Y):\n","    max_vals, max_indices = torch.max(X, 1)\n","    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n","    return train_acc\n","\n","def softmax(vals, idx):\n","    valscpu = vals.detach().cpu().squeeze(0)\n","    a = 0\n","    for i in valscpu:\n","        a += np.exp(i)\n","    return ((np.exp(valscpu[idx]))/a).item() * 100\n","\n","\n","def testModel(model, seq):\n","    cate = [\"ê³±ì°½\",\"êµ­ìˆ˜\",\"ëˆì¹´ì¸ \", \"ë””ì €íŠ¸\",\"ë¼ë©˜\",\"ë²„ê±°\", \"ë² ì´ì»¤ë¦¬\", \"ë¶„ì‹\", \"ìŠ¤ì‹œ\", \"ì•„ì‹œì•„ìŒì‹\", \"ì–‘ì‹\", \"ì „ê³¨\", \"ì¤‘ì‹\", \"ì¹˜í‚¨\", \"íƒ€ì½”\", \"í•œì‹\", \"í•´ì‚°ë¬¼\"]\n","    tmp = [seq]\n","    transform = nlp.data.BERTSentenceTransform(tok, max_len, pad=True, pair=False)\n","    tokenized = transform(tmp)\n","\n","    modelload.eval()\n","    result = modelload(torch.tensor([tokenized[0]]).to(device), [tokenized[1]], torch.tensor(tokenized[2]).to(device)) \n","    idx = result.argmax().cpu().item() #ì¶œë ¥ì˜ ìµœëŒ€ê°’ì´ ë‚˜ì˜¤ê²Œí•¨\n","    result2 = F.softmax(result, dim=1).sort() #ê° ê°’ì— ëŒ€í•œ softmaxí•¨ìˆ˜ ì ìš©\n","\n","    #return cate[idx], softmax(result,idx)\n","    return cate[result2[1][0][-1]],round((result2[0][0][-1]).item(), 4)*100, cate[result2[1][0][-2]],round((result2[0][0][-2]).item(), 4)*100, cate[result2[1][0][-3]],round((result2[0][0][-3]).item(), 4)*100\n","\n","# ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ì„œ í•œë²ˆë§Œ ë¡œë“œí•˜ê³  ìºì‹œì— ì €ì¥í•˜ê¸°\n","@st.cache_resource\n","def cache_model(path, modelname):\n","    #modelload = torch.load(\"/content/drive/MyDrive/Colab Notebooks/Save_data/á„†á…©á„ƒá…¦á†¯á„‡á…®á†«á„‰á…¥á†¨/á„á…©á„‡á…¥á„á…³/model6.pt\", map_location=torch.device('cpu')) # cpuì‚¬ìš©ì‹œ\n","    modelload = torch.load(\"/content/drive/MyDrive/Colab Notebooks/main_project/model6.pt\") # gpuì‚¬ìš©ì‹œ\n","    modelload.eval()\n","    return modelload\n","\n","modelload = cache_model('/content/drive/MyDrive/Colab Notebooks/main_project/','model6.pt')\n","\n","# ì¹´ì¹´ì˜¤ api\n","@st.cache_resource\n","def elec_location(region,page_num):\n","    url = 'https://dapi.kakao.com/v2/local/search/keyword.json'\n","    params = {'query': region,'page': page_num, 'sort' : 'popularity'}\n","    headers = {\"Authorization\": \"KakaoAK 6dd31dbd3f7b90aed3f5591fdde29527\"}\n","\n","    places = requests.get(url, params=params, headers=headers).json()['documents']\n","\n","    return places\n","\n","def elec_info(places):\n","    X = []\n","    Y = []\n","    stores = []\n","    road_address = []\n","    phone = []\n","    place_url = []\n","    ID = []\n","    for place in places:\n","        X.append(float(place['x']))\n","        Y.append(float(place['y']))\n","        stores.append(place['place_name'])\n","        road_address.append(place['road_address_name'])\n","        phone.append(place['phone'])\n","        place_url.append(place['place_url'])\n","        ID.append(place['id'])\n","\n","    ar = np.array([ID,stores, X, Y, road_address, phone, place_url]).T\n","    df = pd.DataFrame(ar, columns = ['ID','stores', 'X', 'Y','road_address','phone','place_url'])\n","    return df\n","\n","def keywords(location_name):\n","    df = None\n","    page_num = int(1)\n","    for loca in location_name:\n","        for page in range(1,page_num+1):\n","            local_name = elec_location(loca, page)\n","            local_elec_info = elec_info(local_name)\n","\n","            if df is None:\n","                df = local_elec_info\n","            elif local_elec_info is None:\n","                continue\n","            else:\n","                df = pd.concat([df, local_elec_info],join='outer', ignore_index = True)\n","    return df\n","\n","def make_map(dfs, m):\n","    \n","    minimap = MiniMap() \n","    m.add_child(minimap)\n","\n","    for i in range(len(dfs)):\n","        folium.Marker([dfs['Y'][i],dfs['X'][i]],\n","                      tooltip=dfs['stores'][i],\n","                      popup = '<iframe width=\"800\" height=\"400\" src=\"' + df['place_url'][i] + '\"title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay;  clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>',\n","                      ).add_to(m)\n","    return m\n","\n","#######################################################################################################\n","\n","st.sidebar.header('Side Menu')\n","tab1, tab2 = st.tabs(['search', 'map'])\n","\n","# user ì…ë ¥ê°’ ì €ì¥\n","if 'user_input' not in st.session_state:\n","    st.session_state['user_input'] = ''\n","\n","if 'user_location_input' not in st.session_state:\n","    st.session_state['user_location_input'] = ''\n","\n","with st.sidebar:\n","        when = st.selectbox('ì‹ì‚¬ ì‹œê°„ì€ ì–¸ì œì¸ê°€ìš”?', ['ì•„ì¹¨', 'ì ì‹¬', 'ì €ë…'])\n","        #location = st.text_input('ì§€ê¸ˆ ê³„ì‹  ì§€ì—­ì€ ì–´ë””ì¸ê°€ìš”?', value = '', placeholder = 'ê·¼ì²˜ ì§€í•˜ì²  ì—­ì„ ì…ë ¥í•´ì£¼ì„¸ìš”', key='user_location_input')\n","\n","with tab1:\n","    st.subheader('ğŸ’­ì˜¤ëŠ˜ë„ ë¬´ì—‡ì„ ë¨¹ì„ì§€ ê³ ë¯¼í•˜ê³  ê³„ì‹ ê°€ìš”?')\n","\n","    value = st.text_area('ì§€ê¸ˆ ìƒê°ë‚˜ëŠ” í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ê³  Ctrl+Enterë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”!', placeholder = 'Ex) ìœ¡ì¦™ì´ íŒ¡íŒ¡ í„°ì§€ëŠ” ê³ ì†Œí•œ ìŒì‹ì´ ë¨¹ê³ ì‹¶ì–´.', key='user_input')\n","    cat1,val1, cat2,val2, cat3,val3 = testModel(model ,st.session_state.user_input)\n","\n","    if value:\n","       st.header(cat1, 'ì´ ìŒì‹ì€ ì–´ë– ì‹ ê°€ìš”?')\n","\n","       st.subheader(f\"{cat1}ì´(ê°€) ê°€ì¥ ì í•©í•œ ìŒì‹ì…ë‹ˆë‹¤. ì‹ ë¢°ë„ëŠ” {round(val1, 2)}% ì…ë‹ˆë‹¤.\")\n","       #st.write(cat1, 'ì´(ê°€) ê°€ì¥ ì í•©í•œ ìŒì‹ì…ë‹ˆë‹¤.', 'ì‹ ë¢°ë„ëŠ”', round(val1, 2), '% ì…ë‹ˆë‹¤.')\n","\n","       st.write('ì…ë ¥ë¬¸ì¥ê³¼ ê°€ì¥ ì¼ì¹˜í•˜ëŠ” ìŒì‹ TOP3 ì…ë‹ˆë‹¤.')\n","       st.write('ğŸ¥‡',cat1, 'ì‹ ë¢°ë„ëŠ”', round(val1, 2),'% ì…ë‹ˆë‹¤.')\n","       st.write('ğŸ¥ˆ',cat2, 'ì‹ ë¢°ë„ëŠ”', round(val2, 2),'% ì…ë‹ˆë‹¤.')\n","       st.write('ğŸ¥‰',cat3, 'ì‹ ë¢°ë„ëŠ”', round(val3, 2),'% ì…ë‹ˆë‹¤.')\n","\n","\n","with tab2:\n","    st.subheader('ğŸš‡ê°€ì‹œë ¤ëŠ” ì§€ì—­ì´ ì–´ë””ì¸ê°€ìš”?')\n","    location = st.text_input('ì§€í•˜ì² ì—­ì„ ê¸°ë°˜ìœ¼ë¡œ ìŒì‹ì ì„ ì¶”ì²œí•´ë“œë¦½ë‹ˆë‹¤.', value = '', placeholder = 'ê·¼ì²˜ ì§€í•˜ì²  ì—­ì„ ì…ë ¥í•´ì£¼ì„¸ìš”', key='user_location_input')\n","    user_location = st.session_state.user_location_input\n","\n","    if location:\n","        kakao_location = [user_location + ' ' + cat1]\n","        try:\n","          df = keywords(kakao_location)\n","          lat = 0\n","          lon = 0\n","          for i in df['Y']:\n","              lat += float(i)\n","          for j in df['X']:\n","              lon += float(j)\n","          lat = lat/len(df['Y'])\n","          lon = lon/len(df['X'])\n","          m = folium.Map(kakao_location=[lat, lon],   # ê¸°ì¤€ì¢Œí‘œ: current_location\n","                        zoom_start=16)\n","          make_map = make_map(df, m)\n","          st_folium(make_map, width = 1000, height = 500, zoom=16, center = [lat, lon])\n","          df = df.drop(columns = ['ID', 'X', 'Y'])\n","          st.dataframe(df)\n","          st.write('ê²°ê³¼ëŠ” ì¸ê¸°ë„ìˆœìœ¼ë¡œ ë°˜ì˜ë˜ì—ˆìŠµë‹ˆë‹¤.')\n","        except:\n","          st.write('ì•„ì‰½ê²Œë„ ' + user_location + ' ê·¼ì²˜ì—ëŠ” ' + cat1 + ' ê°€ê²Œê°€ ì—†ìŠµë‹ˆë‹¤ã… ã… ')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8GOkWNdSmpbI","outputId":"014685c7-f099-45a8-8fcb-e3dc48b3abd6","executionInfo":{"status":"ok","timestamp":1676120161610,"user_tz":-540,"elapsed":377,"user":{"displayName":"JUNSEOK LEE","userId":"08889545003597910636"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing app2.py\n"]}]},{"cell_type":"code","source":["!cat app2.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j7swO3uYtHgY","outputId":"f72335f5-fa59-4f0f-b989-5d55d4e39829","executionInfo":{"status":"ok","timestamp":1676120168839,"user_tz":-540,"elapsed":652,"user":{"displayName":"JUNSEOK LEE","userId":"08889545003597910636"}}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","import streamlit as st\n","import streamlit.components.v1 as html\n","import numpy as np\n","import pandas as pd\n","import requests\n","import folium\n","from folium.plugins import MiniMap\n","from streamlit_folium import st_folium\n","\n","# torch\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import gluonnlp as nlp\n","import numpy as np\n","from tqdm import tqdm, tqdm_notebook\n","\n","# kobert\n","from kobert.utils import get_tokenizer\n","from kobert.pytorch_kobert import get_pytorch_kobert_model\n","\n","#transformers\n","from transformers import AdamW\n","from transformers.optimization import get_cosine_schedule_with_warmup\n","\n","# í˜ì´ì§€ì˜ ê¸°ë³¸ ì„¤ì • êµ¬ì„±\n","st.set_page_config(\n"," layout=\"wide\",\n"," page_title='ì˜¤ëŠ˜ ì´ê±° ë¨¹ì–´')\n","\n","#######################################################################################################\n","#### ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ####\n","\n","device = torch.device(\"cuda:0\") #GPUì‚¬ìš©\n","#device = torch.device(\"cpu\")  #CPUì‚¬ìš©\n","\n","bertmodel, vocab = get_pytorch_kobert_model()\n","\n","tokenizer = get_tokenizer()\n","tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n","\n","class BERTDataset(Dataset):\n","    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len, pad, pair):\n","        transform = nlp.data.BERTSentenceTransform(\n","            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n","\n","        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n","        self.labels = [np.int32(i[label_idx]) for i in dataset]\n","\n","    def __getitem__(self, i):\n","        return (self.sentences[i] + (self.labels[i], ))\n","\n","    def __len__(self):\n","        return (len(self.labels))\n","\n","max_len = 64\n","batch_size = 64\n","warmup_ratio = 0.1\n","num_epochs = 20\n","max_grad_norm = 1\n","log_interval = 200\n","learning_rate =  5e-5\n","\n","class BERTClassifier(nn.Module):\n","    def __init__(self,\n","                 bert,\n","                 hidden_size = 768,\n","                 num_classes=17,\n","                 dr_rate=None,\n","                 params=None):\n","        super(BERTClassifier, self).__init__()\n","        self.bert = bert\n","        self.dr_rate = dr_rate\n","                 \n","        self.classifier = nn.Linear(hidden_size , num_classes)\n","        if dr_rate:\n","            self.dropout = nn.Dropout(p=dr_rate)\n","    \n","    def gen_attention_mask(self, token_ids, valid_length):\n","\n","        attention_mask = torch.zeros_like(token_ids)\n","        for i, v in enumerate(valid_length):\n","            attention_mask[i][:v] = 1\n","        return attention_mask.float()\n","\n","    def forward(self, token_ids, valid_length, segment_ids):\n","        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n","\n","        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device), return_dict=False)\n","\n","        if self.dr_rate:\n","            out = self.dropout(pooler)\n","        return self.classifier(out)\n","\n","model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)\n","\n","no_decay = ['bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0} ]\n","\n","optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n","loss_fn = nn.CrossEntropyLoss()\n","\n","def calc_accuracy(X,Y):\n","    max_vals, max_indices = torch.max(X, 1)\n","    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n","    return train_acc\n","\n","def softmax(vals, idx):\n","    valscpu = vals.detach().cpu().squeeze(0)\n","    a = 0\n","    for i in valscpu:\n","        a += np.exp(i)\n","    return ((np.exp(valscpu[idx]))/a).item() * 100\n","\n","\n","def testModel(model, seq):\n","    cate = [\"ê³±ì°½\",\"êµ­ìˆ˜\",\"ëˆì¹´ì¸ \", \"ë””ì €íŠ¸\",\"ë¼ë©˜\",\"ë²„ê±°\", \"ë² ì´ì»¤ë¦¬\", \"ë¶„ì‹\", \"ìŠ¤ì‹œ\", \"ì•„ì‹œì•„ìŒì‹\", \"ì–‘ì‹\", \"ì „ê³¨\", \"ì¤‘ì‹\", \"ì¹˜í‚¨\", \"íƒ€ì½”\", \"í•œì‹\", \"í•´ì‚°ë¬¼\"]\n","    tmp = [seq]\n","    transform = nlp.data.BERTSentenceTransform(tok, max_len, pad=True, pair=False)\n","    tokenized = transform(tmp)\n","\n","    modelload.eval()\n","    result = modelload(torch.tensor([tokenized[0]]).to(device), [tokenized[1]], torch.tensor(tokenized[2]).to(device)) \n","    idx = result.argmax().cpu().item() #ì¶œë ¥ì˜ ìµœëŒ€ê°’ì´ ë‚˜ì˜¤ê²Œí•¨\n","    result2 = F.softmax(result, dim=1).sort() #ê° ê°’ì— ëŒ€í•œ softmaxí•¨ìˆ˜ ì ìš©\n","\n","    #return cate[idx], softmax(result,idx)\n","    return cate[result2[1][0][-1]],round((result2[0][0][-1]).item(), 4)*100, cate[result2[1][0][-2]],round((result2[0][0][-2]).item(), 4)*100, cate[result2[1][0][-3]],round((result2[0][0][-3]).item(), 4)*100\n","\n","# ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ì„œ í•œë²ˆë§Œ ë¡œë“œí•˜ê³  ìºì‹œì— ì €ì¥í•˜ê¸°\n","@st.cache_resource\n","def cache_model(path, modelname):\n","    #modelload = torch.load(\"/content/drive/MyDrive/Colab Notebooks/Save_data/á„†á…©á„ƒá…¦á†¯á„‡á…®á†«á„‰á…¥á†¨/á„á…©á„‡á…¥á„á…³/model6.pt\", map_location=torch.device('cpu')) # cpuì‚¬ìš©ì‹œ\n","    modelload = torch.load(\"/content/drive/MyDrive/Colab Notebooks/main_project/model6.pt\") # gpuì‚¬ìš©ì‹œ\n","    modelload.eval()\n","    return modelload\n","\n","modelload = cache_model('/content/drive/MyDrive/Colab Notebooks/main_project/','model6.pt')\n","\n","# ì¹´ì¹´ì˜¤ api\n","@st.cache_resource\n","def elec_location(region,page_num):\n","    url = 'https://dapi.kakao.com/v2/local/search/keyword.json'\n","    params = {'query': region,'page': page_num, 'sort' : 'popularity'}\n","    headers = {\"Authorization\": \"KakaoAK 6dd31dbd3f7b90aed3f5591fdde29527\"}\n","\n","    places = requests.get(url, params=params, headers=headers).json()['documents']\n","\n","    return places\n","\n","def elec_info(places):\n","    X = []\n","    Y = []\n","    stores = []\n","    road_address = []\n","    phone = []\n","    place_url = []\n","    ID = []\n","    for place in places:\n","        X.append(float(place['x']))\n","        Y.append(float(place['y']))\n","        stores.append(place['place_name'])\n","        road_address.append(place['road_address_name'])\n","        phone.append(place['phone'])\n","        place_url.append(place['place_url'])\n","        ID.append(place['id'])\n","\n","    ar = np.array([ID,stores, X, Y, road_address, phone, place_url]).T\n","    df = pd.DataFrame(ar, columns = ['ID','stores', 'X', 'Y','road_address','phone','place_url'])\n","    return df\n","\n","def keywords(location_name):\n","    df = None\n","    page_num = int(1)\n","    for loca in location_name:\n","        for page in range(1,page_num+1):\n","            local_name = elec_location(loca, page)\n","            local_elec_info = elec_info(local_name)\n","\n","            if df is None:\n","                df = local_elec_info\n","            elif local_elec_info is None:\n","                continue\n","            else:\n","                df = pd.concat([df, local_elec_info],join='outer', ignore_index = True)\n","    return df\n","\n","def make_map(dfs, m):\n","    \n","    minimap = MiniMap() \n","    m.add_child(minimap)\n","\n","    for i in range(len(dfs)):\n","        folium.Marker([dfs['Y'][i],dfs['X'][i]],\n","                      tooltip=dfs['stores'][i],\n","                      popup = '<iframe width=\"800\" height=\"400\" src=\"' + df['place_url'][i] + '\"title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay;  clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>',\n","                      ).add_to(m)\n","    return m\n","\n","#######################################################################################################\n","\n","st.sidebar.header('Side Menu')\n","tab1, tab2 = st.tabs(['search', 'map'])\n","\n","# user ì…ë ¥ê°’ ì €ì¥\n","if 'user_input' not in st.session_state:\n","    st.session_state['user_input'] = ''\n","\n","if 'user_location_input' not in st.session_state:\n","    st.session_state['user_location_input'] = ''\n","\n","with st.sidebar:\n","        when = st.selectbox('ì‹ì‚¬ ì‹œê°„ì€ ì–¸ì œì¸ê°€ìš”?', ['ì•„ì¹¨', 'ì ì‹¬', 'ì €ë…'])\n","        #location = st.text_input('ì§€ê¸ˆ ê³„ì‹  ì§€ì—­ì€ ì–´ë””ì¸ê°€ìš”?', value = '', placeholder = 'ê·¼ì²˜ ì§€í•˜ì²  ì—­ì„ ì…ë ¥í•´ì£¼ì„¸ìš”', key='user_location_input')\n","\n","with tab1:\n","    st.subheader('ğŸ’­ì˜¤ëŠ˜ë„ ë¬´ì—‡ì„ ë¨¹ì„ì§€ ê³ ë¯¼í•˜ê³  ê³„ì‹ ê°€ìš”?')\n","\n","    value = st.text_area('ì§€ê¸ˆ ìƒê°ë‚˜ëŠ” í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ê³  Ctrl+Enterë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”!', placeholder = 'Ex) ìœ¡ì¦™ì´ íŒ¡íŒ¡ í„°ì§€ëŠ” ê³ ì†Œí•œ ìŒì‹ì´ ë¨¹ê³ ì‹¶ì–´.', key='user_input')\n","    cat1,val1, cat2,val2, cat3,val3 = testModel(model ,st.session_state.user_input)\n","\n","    if value:\n","       st.header(cat1, 'ì´ ìŒì‹ì€ ì–´ë– ì‹ ê°€ìš”?')\n","\n","       st.subheader(f\"{cat1}ì´(ê°€) ê°€ì¥ ì í•©í•œ ìŒì‹ì…ë‹ˆë‹¤. ì‹ ë¢°ë„ëŠ” {round(val1, 2)}% ì…ë‹ˆë‹¤.\")\n","       #st.write(cat1, 'ì´(ê°€) ê°€ì¥ ì í•©í•œ ìŒì‹ì…ë‹ˆë‹¤.', 'ì‹ ë¢°ë„ëŠ”', round(val1, 2), '% ì…ë‹ˆë‹¤.')\n","\n","       st.write('ì…ë ¥ë¬¸ì¥ê³¼ ê°€ì¥ ì¼ì¹˜í•˜ëŠ” ìŒì‹ TOP3 ì…ë‹ˆë‹¤.')\n","       st.write('ğŸ¥‡',cat1, 'ì‹ ë¢°ë„ëŠ”', round(val1, 2),'% ì…ë‹ˆë‹¤.')\n","       st.write('ğŸ¥ˆ',cat2, 'ì‹ ë¢°ë„ëŠ”', round(val2, 2),'% ì…ë‹ˆë‹¤.')\n","       st.write('ğŸ¥‰',cat3, 'ì‹ ë¢°ë„ëŠ”', round(val3, 2),'% ì…ë‹ˆë‹¤.')\n","\n","\n","with tab2:\n","    st.subheader('ğŸš‡ê°€ì‹œë ¤ëŠ” ì§€ì—­ì´ ì–´ë””ì¸ê°€ìš”?')\n","    location = st.text_input('ì§€í•˜ì² ì—­ì„ ê¸°ë°˜ìœ¼ë¡œ ìŒì‹ì ì„ ì¶”ì²œí•´ë“œë¦½ë‹ˆë‹¤.', value = '', placeholder = 'ê·¼ì²˜ ì§€í•˜ì²  ì—­ì„ ì…ë ¥í•´ì£¼ì„¸ìš”', key='user_location_input')\n","    user_location = st.session_state.user_location_input\n","\n","    if location:\n","        kakao_location = [user_location + ' ' + cat1]\n","        try:\n","          df = keywords(kakao_location)\n","          lat = 0\n","          lon = 0\n","          for i in df['Y']:\n","              lat += float(i)\n","          for j in df['X']:\n","              lon += float(j)\n","          lat = lat/len(df['Y'])\n","          lon = lon/len(df['X'])\n","          m = folium.Map(kakao_location=[lat, lon],   # ê¸°ì¤€ì¢Œí‘œ: current_location\n","                        zoom_start=16)\n","          make_map = make_map(df, m)\n","          st_folium(make_map, width = 1000, height = 500, zoom=16, center = [lat, lon])\n","          df = df.drop(columns = ['ID', 'X', 'Y'])\n","          st.dataframe(df)\n","          st.write('ê²°ê³¼ëŠ” ì¸ê¸°ë„ìˆœìœ¼ë¡œ ë°˜ì˜ë˜ì—ˆìŠµë‹ˆë‹¤.')\n","        except:\n","          st.write('ì•„ì‰½ê²Œë„ ' + user_location + ' ê·¼ì²˜ì—ëŠ” ' + cat1 + ' ê°€ê²Œê°€ ì—†ìŠµë‹ˆë‹¤ã… ã… ')\n"]}]},{"cell_type":"markdown","source":["# streamlit run"],"metadata":{"id":"CrQjfYxwN8KX"}},{"cell_type":"code","source":["!nohup streamlit run app2.py --server.port 80 &"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QzNZCMGBJxkQ","outputId":"b2ef61ad-82a8-40a4-8a76-bd2f3d821577","executionInfo":{"status":"ok","timestamp":1676120172227,"user_tz":-540,"elapsed":7,"user":{"displayName":"JUNSEOK LEE","userId":"08889545003597910636"}}},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["nohup: appending output to 'nohup.out'\n"]}]},{"cell_type":"markdown","source":["# ngrok ì—°ê²°í•´ì„œ ì£¼ì†Œ ë°›ê¸°"],"metadata":{"id":"Qz34p3i1OA3R"}},{"cell_type":"code","source":["url = ngrok.connect(port='80')\n","url"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FQ_LI3eNJ5Eo","outputId":"c4d970a3-b461-43d7-9b98-61fb1e642486","executionInfo":{"status":"ok","timestamp":1676120177492,"user_tz":-540,"elapsed":875,"user":{"displayName":"JUNSEOK LEE","userId":"08889545003597910636"}}},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<NgrokTunnel: \"http://67a3-34-90-246-178.ngrok.io\" -> \"http://localhost:80\">"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["# ìƒˆ ì£¼ì†Œ ë°›ì„ë•Œ kill í•˜ê¸°"],"metadata":{"id":"3SSkgD37OHm3"}},{"cell_type":"code","source":["ngrok.kill()"],"metadata":{"id":"h9ZrUmTJKHhv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3t8Ghyvnp-_V"},"execution_count":null,"outputs":[]}]}